{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = '../../news-edits-dbs/'\n",
    "for download_part in glob.glob(\"/Users/spangher/Downloads/public NewsEdits*/\"):\n",
    "    for folder in glob.glob(download_part + '/*'):\n",
    "        folder_name = os.path.basename(folder)\n",
    "        for file_path in glob.glob(os.path.join(download_part, folder_name, '*')):\n",
    "            file_name = os.path.basename(file_path)\n",
    "            shutil.move(file_path, os.path.join(dest_path, folder_name, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import boto3\n",
    "from boto.mturk.connection import MTurkConnection\n",
    "from boto.mturk.question import HTMLQuestion\n",
    "from boto.mturk.question import ExternalQuestion\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from boto.mturk.qualification import (\n",
    "    Qualifications,\n",
    "    PercentAssignmentsApprovedRequirement, \n",
    "    NumberHitsApprovedRequirement\n",
    ")\n",
    "\n",
    "import sys, os, json, re\n",
    "sys.path.insert(0, '../')\n",
    "from mturk import mturk_handler as um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util_data_fetching_for_app as uda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "category_model = fasttext.load_model('../scripts/fasttext_model__news-classification.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new edits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pathway= '../../news-edits-dbs/matched-sentences/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip $db_pathway/ap-matched-sentences.db.gz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(db_pathway, 'bbc-2-matched-sentences.db')\n",
    "db_path = os.path.join(db_pathway, 'ap-matched-sentences.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "doc_level_stats_df = pd.read_sql('select * from doc_level_stats', con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_versions = doc_level_stats_df['entry_id'].value_counts().to_frame('num_versions')\n",
    "candidate_articles = (\n",
    "    doc_level_stats_df\n",
    "    .merge(num_versions, left_on='entry_id', right_index=True)\n",
    "#     .loc[lambda df: (df['version_x'] / df['num_versions']) < .2]  ## articles early in their lifecycle\n",
    "    .loc[lambda df: df['num_sentences_x'] < 25]                     ## short articles\n",
    "    .loc[lambda df: df['num_sentences_y'] < 35]                     ## short articles    \n",
    "    .loc[lambda df: (df['num_edited_x'] / df['num_sentences_x']) > .3]                     ## short articles    \n",
    "    .loc[lambda df: (df['num_added'] / df['num_sentences_x']) > .2] ## articles that grow between iterations\n",
    "    .loc[lambda df: (df['num_added'] / df['num_sentences_x']) < .4] ## articles that grow between iterations    \n",
    "    .loc[lambda df: df['num_deleted'] / df['num_sentences_x'] < .05]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = uda.get_join_keys(candidate_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents, split_sents = uda.get_data_from_sqlite_by_sentence_criteria('bbc', conn, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_mturk = uda.match_sentences(matched_sents, split_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_mturk['category'] = (df_for_mturk\n",
    " .apply(lambda x: list(filter(lambda y: y['version'] == x.name[2], x['nodes'])), axis=1 )\n",
    " .apply(lambda x: list(map(lambda y: y['sentence'], x)))\n",
    " .apply(lambda x: category_model.predict(' '.join(x)))\n",
    " .apply(lambda x: x[0][0].replace('__label__', ''))\n",
    ")\n",
    "\n",
    "df_for_mturk = df_for_mturk.loc[lambda df: df['category'] != 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_annotate = (df_for_mturk\n",
    " .reset_index()\n",
    " .loc[lambda df: df['version_x'] < 6]\n",
    " .groupby('version_x')\n",
    " .apply(lambda x: x.sample(n=50).squeeze() if len(x) > 50 else x.squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_annotate_json = uda.dump_output_to_app_readable(sample_to_annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app/data/ap-sampled-data.json', 'w') as f:\n",
    "    json.dump(to_annotate_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grant permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from importlib import reload\n",
    "reload(um)\n",
    "\n",
    "env = 'sandbox'\n",
    "# env = 'production'\n",
    "mturk = um.MTurkHandler(environment=env) #=production/sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mturk.create_qualification('edit-intention editor', 'text-classification, journalism, editing', 'Ability to identify edit intentions.')\n",
    "prod_edit = {\n",
    "    'QualificationTypeId': '3ZSPHI510KUYCVE8V8R0HC9UR00IJW',\n",
    "    'CreationTime': datetime.datetime(2023, 2, 2, 11, 46, 23),\n",
    "    'Name': 'edit-intention editor',\n",
    "    'Description': 'Ability to identify edit intentions.',\n",
    "    'Keywords': 'text-classification, journalism, editing',\n",
    "    'QualificationTypeStatus': 'Active',\n",
    "    'IsRequestable': True,\n",
    "    'AutoGranted': False\n",
    "}\n",
    "\n",
    "prod_edit = {\n",
    "    'QualificationTypeId': '3TBOPHETGJA0W0O69QILABF56T8L2Q',\n",
    "    'CreationTime': datetime.datetime(2023, 2, 2, 13, 12, 47),\n",
    "    'Name': 'editor v2',\n",
    "    'Description': 'Ability to identify edit intentions.',\n",
    "    'Keywords': 'text-classification, journalism, editing',\n",
    "    'QualificationTypeStatus': 'Active',\n",
    "    'IsRequestable': True,\n",
    "    'AutoGranted': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_QUALIFICATION = True\n",
    "# sandbox editor qualification: 3H3KEN1OLUQQR02IYZSVMYM7ESCBIO\n",
    "# sandbox_journalist_qual = mturk.create_qualification('journalist')\n",
    "# production_journalist_qual = mturk.create_qualification(\n",
    "#     'journalist',\n",
    "#     qual_keywords='journalist, writer, editor', \n",
    "#     qual_description='Turkers with experience in newsrooms performing reporting and editing functions.'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_journalist_qual = {'QualificationType': {'QualificationTypeId': '3YJP8DI8F7IJNZ5SWSN2GXBAZJF4Q2',\n",
    "#   'CreationTime': datetime.datetime(2021, 11, 4, 23, 3, 23, tzinfo=tzlocal()),\n",
    "  'Name': 'journalist',\n",
    "  'Description': 'A custom qualification group given to workers we deem good.',\n",
    "  'Keywords': 'custom-group filtering',\n",
    "  'QualificationTypeStatus': 'Active',\n",
    "  'IsRequestable': True,\n",
    "  'AutoGranted': False},\n",
    " 'ResponseMetadata': {'RequestId': '0edf8c83-69c6-446a-8d20-930712b2efa7',\n",
    "  'HTTPStatusCode': 200,\n",
    "  'HTTPHeaders': {'x-amzn-requestid': '0edf8c83-69c6-446a-8d20-930712b2efa7',\n",
    "   'content-type': 'application/x-amz-json-1.1',\n",
    "   'content-length': '315',\n",
    "   'date': 'Fri, 05 Nov 2021 06:03:22 GMT'},\n",
    "  'RetryAttempts': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox_journalist_qual = {'QualificationType': {'QualificationTypeId': '3H3KEN1OLUQQR02IYZSVMYM7ESCBIO',\n",
    "#   'CreationTime': datetime.datetime(2021, 11, 4, 23, 0, 7, tzinfo=tzlocal()),\n",
    "  'Name': 'journalist',\n",
    "  'Description': 'A custom qualification group given to workers we deem good.',\n",
    "  'Keywords': 'custom-group filtering',\n",
    "  'QualificationTypeStatus': 'Active',\n",
    "  'IsRequestable': True,\n",
    "  'AutoGranted': False},\n",
    " 'ResponseMetadata': {'RequestId': '28302a92-3fef-47d4-8f02-1b93f1e08258',\n",
    "  'HTTPStatusCode': 200,\n",
    "  'HTTPHeaders': {'x-amzn-requestid': '28302a92-3fef-47d4-8f02-1b93f1e08258',\n",
    "   'content-type': 'application/x-amz-json-1.1',\n",
    "   'content-length': '315',\n",
    "   'date': 'Fri, 05 Nov 2021 06:00:07 GMT'},\n",
    "  'RetryAttempts': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = [\n",
    "    'A25JJA15CM494B', #'Margot Williams'\n",
    "    'AEMJM5IATKDU8', #'Beau Collins'\n",
    "    'A200N6ADPZGA9A', #'Eliza Billingham'\n",
    "    'AKLN3G3G3NATU', #'Miacel Spotted Elk'\n",
    "    'ARBK4601C0FHM', #'Matt Zdun'\n",
    "    'A1G0DK990FW3HL', #'Antonio Jarne'\n",
    "    'AL5YEU3OZGP4T', # Marina Villeneuve\n",
    "    'A3SBGUREOSQP6G', # Steeve\n",
    "    'A8SFABNWE84RC'  # steve melendez    \n",
    "]\n",
    "\n",
    "# mturk.give_qualification_to_workers(workers, qualification_id=production_journalist_qual['QualificationType']['QualificationTypeId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk.give_qualification_to_workers(\n",
    "    workers, \n",
    "    qualification_id=prod_edit['QualificationType']['QualificationTypeId']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make file from template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mturk import make_mturk_from_json as mj\n",
    "from importlib import reload\n",
    "reload(mj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import copy\n",
    "\n",
    "def is_quote(x):\n",
    "    s = unidecode.unidecode(x['sentence'])\n",
    "    return s in [\"''\", '\"']\n",
    "\n",
    "def arc_contains_one_of_nodelist(arc, nodes):\n",
    "    contains_one_node = False\n",
    "    for n in nodes:\n",
    "        in_x = (arc['version_x'] == n['version']) and (arc['sent_idx_x'] == n['sent_idx'])\n",
    "        in_y = (arc['version_y'] == n['version']) and (arc['sent_idx_y'] == n['sent_idx'])\n",
    "        contains_one_node |= (in_x or in_y)\n",
    "    return contains_one_node\n",
    "\n",
    "def update_key(d, key, val=None):\n",
    "    \"\"\"Update the dictionary, unless the value is null, then keep the old value.\"\"\"\n",
    "    if val is None:\n",
    "        val = d[key]\n",
    "        \n",
    "    d[key] = val\n",
    "    return d\n",
    "\n",
    "def clean_data(datum):\n",
    "    def _get_old_new_wrapper(nodes_list):\n",
    "        new_idxs = list(range(len(nodes_list)))\n",
    "        old_idxs = list(map(lambda x: x['sent_idx'], nodes_list))\n",
    "        return dict(zip(old_idxs, new_idxs))\n",
    "    \n",
    "    # generic cleaning\n",
    "    datum = copy.deepcopy(datum)\n",
    "    nodes = datum['nodes']\n",
    "    arcs = datum['arcs']\n",
    "    for n in nodes:\n",
    "        n['sentence'] = n['sentence'].replace('\"', '')\n",
    "    datum['nodes'] = nodes\n",
    "    datum['arcs'] = arcs\n",
    "    \n",
    "    # get single-quotes and filter down\n",
    "    quote_nodes = list(filter(is_quote, datum['nodes']))\n",
    "    new_arcs = list(filter(lambda arc: not arc_contains_one_of_nodelist(arc, quote_nodes), datum['arcs']))\n",
    "    new_nodes = list(filter(lambda x: not is_quote(x), datum['nodes']))\n",
    "    \n",
    "    # make version mappers\n",
    "    vers_x = min(list(map(lambda x: x['version'], new_nodes)))\n",
    "    vers_y = max(list(map(lambda x: x['version'], new_nodes)))\n",
    "    new_nodes_x = list(filter(lambda x: x['version'] == vers_x, new_nodes))\n",
    "    new_nodes_y = list(filter(lambda x: x['version'] == vers_y, new_nodes))\n",
    "    x_idx_mapper = _get_old_new_wrapper(new_nodes_x)\n",
    "    y_idx_mapper = _get_old_new_wrapper(new_nodes_y)\n",
    "    \n",
    "    nodes_x = list(map(lambda d: update_key(d, 'sent_idx', x_idx_mapper.get(d['sent_idx'])), new_nodes_x))\n",
    "    nodes_y = list(map(lambda d: update_key(d, 'sent_idx', y_idx_mapper.get(d['sent_idx'])), new_nodes_y))\n",
    "\n",
    "    new_arcs = list(map(lambda d: update_key(d, 'sent_idx_x', x_idx_mapper.get(d['sent_idx_x'])), new_arcs))\n",
    "    new_arcs = list(map(lambda d: update_key(d, 'sent_idx_y', y_idx_mapper.get(d['sent_idx_y'])), new_arcs))\n",
    "\n",
    "    datum['nodes'] = new_nodes_x + new_nodes_y\n",
    "    datum['arcs'] = new_arcs\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../app/data/test_dump_mturk_50.json'\n",
    "\n",
    "with open(data_file) as f:\n",
    "    input_data = json.load(f)\n",
    "\n",
    "PRINT_DETAILS = True\n",
    "keys = list(input_data.keys())\n",
    "if PRINT_DETAILS:\n",
    "    print('sample keys:')\n",
    "    print('\\n'.join(keys[:5]))\n",
    "    print('\\nnum documents:')\n",
    "    print(len(keys))\n",
    "\n",
    "SHUFFLE_DATA = False\n",
    "if SHUFFLE_DATA:\n",
    "    import random\n",
    "    random.shuffle(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_htmls = []\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    # select files\n",
    "    k = keys[i]\n",
    "\n",
    "    # get data\n",
    "    datum = input_data[k]\n",
    "\n",
    "#     instructions_file = '../app/static/assets/instructions.html'\n",
    "#     with open(instructions_file) as f:\n",
    "#         instructions_html = f.read()\n",
    "\n",
    "#     data_id = re.split('\\(\\'|\\'\\, |\\, |\\)', k)\n",
    "#     data_id = list(filter(lambda x: x != '', data_id))\n",
    "#     data_id = '-'.join(data_id)\n",
    "    data_id = k\n",
    "\n",
    "#     output = mj.render_page(\n",
    "#         clean_data(datum),\n",
    "#         data_id,\n",
    "#         write=True, \n",
    "#         template_folder='../app/templates/', \n",
    "#         template_fn='cluster_anno.html',\n",
    "#         output_dir='../mturk/tasks_to_launch',\n",
    "#         instructions=instructions_html\n",
    "#     )\n",
    "\n",
    "    output = mj.render_page(\n",
    "        datum,\n",
    "        data_id,\n",
    "        write=True, \n",
    "        template_folder='../app/templates/', \n",
    "        template_fn='cluster_anno.html',\n",
    "        output_dir='../mturk/tasks_to_launch',\n",
    "#         instructions=instructions_html\n",
    "    )\n",
    "    output_htmls.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_htmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch to MTurk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'sandbox'\n",
    "# env = 'production'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk = um.MTurkHandler(environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_requirements = [{\n",
    "    # 'QualificationTypeId': prod_edit['QualificationTypeId'],  ## journalists\n",
    "    'Comparator': 'GreaterThanOrEqualTo',\n",
    "    'IntegerValues': [90],      \n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_hits = []\n",
    "for output in tqdm(output_htmls):\n",
    "    try:\n",
    "        title = 'News Clustering'\n",
    "        new_hit = mturk.client.create_hit(\n",
    "            Title = title,\n",
    "            Description = 'Cluster News Articles into Packages',\n",
    "            Keywords = 'classification',\n",
    "            Reward = '3.00',\n",
    "            MaxAssignments = 1,\n",
    "            LifetimeInSeconds = 17_280_000,\n",
    "            AssignmentDurationInSeconds = 600_000,\n",
    "            AutoApprovalDelayInSeconds = 14_400,\n",
    "            Question = output['html'],\n",
    "            QualificationRequirements=worker_requirements if env == 'production' else []\n",
    "        )\n",
    "        created_hits.append(new_hit)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(created_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_list = list(map(lambda x: x['HIT']['HITId'], created_hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cache/2023-02-06__fourth-batch-everyone-hit-list.json', 'w') as f:\n",
    "    json.dump(hit_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = mturk.get_answer_df_for_hit_list(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "answer_df['assignment_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df.to_pickle('cache/2023-01-24__first-batch-results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df.drop_duplicates('assignment_id')['time_delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
